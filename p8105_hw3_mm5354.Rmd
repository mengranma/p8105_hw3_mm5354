---
title: "p8105_hw3_mm5354"
author: "Mengran Ma"
date: "2018/10/3"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# install.packages("devtools")
devtools::install_github("p8105/p8105.datasets", force = TRUE)
library(p8105.datasets)
library(tidyverse)
library(ggplot2)
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
```
```{r}
data("brfss_smart2010")
brfss = janitor::clean_names(brfss_smart2010) %>% 
  #Focus on the “Overall Health” topic
  filter(topic == "Overall Health") %>% 
  filter(topic != "Excellent" | topic != "`Very good`" | topic != "Good" | topic != "Fair" | topic != "Poor") %>%
  mutate(., response = as.factor(brfss$response))
```
include only responses from “Excellent” to “Poor” (i.e. no pre-collapsed categories)
organize responses as a factor taking levels from “Excellent” to “Poor”


In 2002, which states were observed at 7 locations?
Make a “spaghetti plot” that shows the number of observations in each state from 2002 to 2010.

```{r}
brfss2002 = filter(brfss, year == 2002)

```

```{r}
Numobservation_2002 = sum(brfss$year == 2002)
Numobservation_2003 = sum(brfss$year == 2003)
Numobservation_2004 = sum(brfss$year == 2004)
Numobservation_2005 = sum(brfss$year == 2005)
Numobservation_2006 = sum(brfss$year == 2006)
Numobservation_2007 = sum(brfss$year == 2007)
Numobservation_2008 = sum(brfss$year == 2008)
Numobservation_2009 = sum(brfss$year == 2009)
Numobservation_2010 = sum(brfss$year == 2010)
Numobservation_eachyear = c(Numobservation_2002, Numobservation_2003, Numobservation_2004, Numobservation_2005, Numobservation_2006, Numobservation_2007, Numobservation_2008, Numobservation_2009, Numobservation_2010)
plot(aes(x = factor("2002", "2003", "2004", "2005", "2006", "2007", "2008", "2009", "2010"), y =factor(Numobservation_eachyear))) 
```

Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.
```{r}
brfss_Excellent = filter(brfss, brfss$locationabbr == "NY") 
proportion_Excellent = sum(brfss2002Excellent$response == "Excellent")/nrow(brfss2002Excellent)

instacart_data %>%
  group_by(year, locationdesc) %>%
  summarize(mean_tmax = mean(tmax)) %>% 
  spread(key = month, value = mean_tmax) %>% 
```

For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time.
```{r}
for(brfss$year in 2002:2010){
  for (brfss$locationabbr in .){
}
}
```


```{r, include= FALSE}
# install.packages("devtools")
devtools::install_github("p8105/p8105.datasets", force = TRUE)
library(p8105.datasets)
```

```{r}
data(instacart)
instacart_data = janitor::clean_names(instacart)
```
Write a short description of the dataset, noting the size and structure of the data, which is dimension: `r nrow(instacart_data)` rows x `r ncol(instacart_data)` columns. Describing some key variables, and giving illstrative examples of observations. 

How many aisles are there, and which aisles are the most items ordered from?
```{r}
length(instacart_data$aisle)
names(which.max(table(instacart_data$aisle)))
```

Make a plot that shows the number of items ordered in each aisle. Order aisles sensibly, and organize your plot so others can read it.
```{r}
ggplot(instacart_data, aes(x = aisle_id)) + 
  geom_bar()
```

Make a table showing the most popular item aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”

```{r}
baking_ingred = filter(instacart_data, aisle == "baking ingredients")
dog_food_care = filter(instacart_data, aisle == "dog food care")
packaged_veg_fruits = filter(instacart_data, aisle == "packaged vegetables fruits")
```

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).


